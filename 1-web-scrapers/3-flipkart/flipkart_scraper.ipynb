{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d07e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf6eb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_with_splash(url,wait=2):\n",
    "    params = {\n",
    "        'url': url,\n",
    "        'wait': wait\n",
    "    }\n",
    "    r = requests.get('http://localhost:8050/render.html', params=params)\n",
    "    if r.status_code == 200:\n",
    "        return r.text\n",
    "    else:\n",
    "        print(f\"Error fetching {url} | Status: {r.status_code}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5536ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('flipkart_urls.txt', 'r') as f:\n",
    "    lines = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b734ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews_data = []\n",
    "\n",
    "\n",
    "for i in range(0, len(lines), 2):\n",
    "    product_url = lines[i]\n",
    "    product_name = lines[i+1]\n",
    "    print(f\"\\nStarting scraping reviews for: {product_name}\")\n",
    "\n",
    "    \n",
    "    html = fetch_with_splash(product_url)\n",
    "    if not html:\n",
    "        print(f\"Failed to fetch product page for {product_name}. Skipping.\")\n",
    "        continue\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    divs = soup.findAll('div', class_=\"col pPAw9M\")\n",
    "    if not divs:\n",
    "        print(f\"No review link container found for {product_name}. Skipping.\")\n",
    "        continue\n",
    "    a_tags = divs[-1].find_all('a')\n",
    "    if not a_tags:\n",
    "        print(f\"No review links found for {product_name}. Skipping.\")\n",
    "        continue\n",
    "    href = a_tags[-1].get('href')\n",
    "    base_url = 'https://www.flipkart.com'\n",
    "    review_base_url = base_url + href\n",
    "\n",
    "    reviews_data = []\n",
    "\n",
    "    \n",
    "    for page_num in range(1, 11):\n",
    "        page_url = f\"{review_base_url}&page={page_num}\"\n",
    "        html = fetch_with_splash(page_url)\n",
    "        if not html:\n",
    "            print(f\"Skipping page {page_num} due to fetch error.\")\n",
    "            continue\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        \n",
    "        review_blocks = soup.findAll(\"div\", {'class': \"ZmyHeo\"})\n",
    "        if not review_blocks:\n",
    "            print(f\"No reviews found on page {page_num}. Stopping pagination.\")\n",
    "            break\n",
    "\n",
    "        for block in review_blocks:\n",
    "            review_text = None\n",
    "            first_div_child = block.find(\"div\")\n",
    "            if first_div_child:\n",
    "                sec_div_child = first_div_child.find(\"div\")\n",
    "                if sec_div_child:\n",
    "                    review_text = sec_div_child.text.strip()\n",
    "\n",
    "            date_text = None\n",
    "            p_tags = block.find_all(\"p\", {'class': \"_2NsDsF\"})\n",
    "            for p in p_tags:\n",
    "                txt = p.text.strip()\n",
    "                if re.match(r'^[a-zA-Z]{3}, \\d{4}$', txt) or re.match(r'^\\d+\\s+\\w+\\s+ago$', txt):\n",
    "                    date_text = txt # Jul, 2024 or 3 days ago\n",
    "                    break\n",
    "\n",
    "            \n",
    "            rating_text = None\n",
    "            col_div = block.find(\"div\", {'class': \"col EPCmJX Ma1fCG\"})\n",
    "            if col_div:\n",
    "                n_sub = col_div.find(\"div\", {'class': \"row\"})\n",
    "                if n_sub:\n",
    "                    n_sub_2 = n_sub.find(\"div\", {'class': \"XQDdHH\"})\n",
    "                    if n_sub_2:\n",
    "                        rating_text = n_sub_2.text.strip()\n",
    "\n",
    "            if review_text:\n",
    "                reviews_data.append({\n",
    "                    'Product': product_name,\n",
    "                    'Review': review_text,\n",
    "                    'Date': date_text,\n",
    "                    'Rating': rating_text\n",
    "                })\n",
    "\n",
    "        print(f\"Scraped {len(review_blocks)} reviews from page {page_num}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    print(f\"Finished scraping {len(reviews_data)} reviews for {product_name}\")\n",
    "    all_reviews_data.extend(reviews_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda2abcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = '../data/flipkart.csv'\n",
    "\n",
    "\n",
    "df = pd.DataFrame(all_reviews_data)\n",
    "if not df.empty:\n",
    "    df.to_csv(output_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
