{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/train_data.csv')\n",
    "test_df = pd.read_csv('../data/test_data.csv')\n",
    "\n",
    "\n",
    "label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n",
    "train_df['Sentiment'] = train_df['Sentiment'].map(label_map)\n",
    "test_df['Sentiment'] = test_df['Sentiment'].map(label_map)\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['Review'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.rename_column(\"Sentiment\", \"labels\")\n",
    "test_dataset = test_dataset.rename_column(\"Sentiment\", \"labels\")\n",
    "\n",
    "\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=12,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=2e-5\n",
    ")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average='weighted')\n",
    "    acc = accuracy_score(p.label_ids, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 178/178 [5:35<00:00,  1.88s/it]\n",
      "Testing Accuracy: 0.9660\n",
      "Testing F1 Score: 0.9664\n",
      "Testing Recall: 0.9660\n",
      "Testing Precision: 0.9669\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate(test_dataset)\n",
    "print(f\"Testing Accuracy: {results['eval_accuracy']:.4f}\")\n",
    "print(f\"Testing F1 Score: {results['eval_f1']:.4f}\")\n",
    "print(f\"Testing Recall: {results['eval_recall']:.4f}\")\n",
    "print(f\"Testing Precision: {results['eval_precision']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '../model/roberta'\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "model.save_pretrained(directory)\n",
    "tokenizer.save_pretrained(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '../model/roberta'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(directory)\n",
    "model = RobertaForSequenceClassification.from_pretrained(directory)\n",
    "\n",
    "sample_review = \"0% quality , bo touch work total pass waste phone\"\n",
    "inputs = tokenizer(sample_review, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "predictions = np.argmax(outputs.logits.detach().numpy(), axis=1)\n",
    "\n",
    "label_map = {2: 'positive', 1: 'neutral', 0: 'negative'}\n",
    "predicted_sentiment = label_map[predictions[0]]\n",
    "print(f\"Predicted sentiment: {predicted_sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_strings = [\n",
    "    \"This product is amazing!\",\n",
    "    \"disappoint with this purchase\",\n",
    "    \"Value for money\",\n",
    "    \"bad\",\n",
    "    \"Great value for the price\",\n",
    "    \"Product worse\",\n",
    "    \"Sucks, I wanna die\",\n",
    "    \"I want to get another one its so good\",\n",
    "    \"Worse\",\n",
    "    \"sometim game answer question correctli alexa say got wrong answer like turn dont light away home\",\n",
    "    \"abl\",\n",
    "    \"Not bad\",\n",
    "    \"Good\",\n",
    "    \"Sure, the movie wasn't *awful*, but it was far from a masterpiece.\",\n",
    "    \"I can't believe they won the game! They totally choked in the last quarter, though.\",\n",
    "    \"Don't get me wrong, the food was good, but the service was painfully slow.\",\n",
    "    \"They say they improved the product, but I haven't noticed a difference yet.\",\n",
    "    \"Lucky me, I found a parking spot right in front of the store.\",\n",
    "    \"It's whatever. I guess the movie was okay.\",\n",
    "    \"That was a close one! Glad we pulled through in the end.\",\n",
    "    \"Eye roll. This new update is just a bunch of bugs.\",\n",
    "    \"Not bad for a first try! I can see potential here.\",\n",
    "    \"While the graphics were impressive, the story felt a bit lacking.\"\n",
    "]\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('../model/roberta')\n",
    "model = RobertaForSequenceClassification.from_pretrained('../model/roberta')\n",
    "\n",
    "def predict_sentiment(model, tokenizer, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    predictions = np.argmax(outputs.logits.detach().numpy(), axis=1)\n",
    "    return predictions[0]\n",
    "\n",
    "label_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "\n",
    "for text in test_strings:\n",
    "    sentiment_label = predict_sentiment(model, tokenizer, text)\n",
    "    predicted_sentiment = label_map[sentiment_label]\n",
    "    print(f\"Review: {text}\")\n",
    "    print(f\"Predicted sentiment: {predicted_sentiment}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../model/roberta'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "label_map = {2: 'positive', 1: 'neutral', 0: 'negative'}\n",
    "\n",
    "df = pd.read_csv('data/products.csv')\n",
    "\n",
    "def get_sentiment(text):\n",
    "    inputs = tokenizer(str(text), return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    pred = np.argmax(outputs.logits.detach().numpy(), axis=1)\n",
    "    return pred[0]\n",
    "\n",
    "df['Sentiment'] = df['Review'].apply(get_sentiment)\n",
    "\n",
    "df.to_csv('products_sentiment.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
